{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the datasets\n",
        "calls = pd.read_csv('calls.csv')\n",
        "customers = pd.read_csv('customers.csv')\n",
        "reasons = pd.read_csv('reason.csv')\n",
        "sentiment_statistics = pd.read_csv('sentiment_statistics.csv')\n",
        "# test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Step 2: Data Cleaning (removing rows with null values)\n",
        "calls.dropna(inplace=True)\n",
        "reasons.dropna(inplace=True)\n",
        "sentiment_statistics.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "g6CDW97nFSee"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to clean spaces\n",
        "def clean_spaces(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove leading and trailing spaces\n",
        "        text = text.strip()\n",
        "        # Replace multiple spaces with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the primary_call_reason column\n",
        "reasons['primary_call_reason'] = reasons['primary_call_reason'].apply(clean_spaces)\n",
        "\n",
        "# Mapping dictionary for similar phrases\n",
        "mapping = {\n",
        "    'Post Flight': 'Post-Flight',\n",
        "    'Products & Services': 'Products and Services',\n",
        "    'Check In': 'Check-In'\n",
        "}\n",
        "\n",
        "# Replace similar reasons using the mapping\n",
        "reasons['primary_call_reason'] = reasons['primary_call_reason'].replace(mapping)\n",
        "\n",
        "# Display the cleaned reasons\n",
        "print(\"Cleaned Primary Call Reasons:\")\n",
        "print(reasons)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGeTL7z9FTRj",
        "outputId": "5aa2a5bf-ca3f-4159-89ad-32628fd33f79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Primary Call Reasons:\n",
            "          call_id primary_call_reason\n",
            "0      4667960400    Voluntary Cancel\n",
            "1      1122072124             Booking\n",
            "2      6834291559              IRROPS\n",
            "3      2266439882             Upgrade\n",
            "4      1211603231             Seating\n",
            "...           ...                 ...\n",
            "66648  7569738090        Mileage Plus\n",
            "66649  1563273072         Post-Flight\n",
            "66650  8865997781             Upgrade\n",
            "66651  8019240181             Upgrade\n",
            "66652  8210720833     Digital Support\n",
            "\n",
            "[66653 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Merge datasets (on 'call_id' and other keys)\n",
        "merged_data = pd.merge(calls, reasons, on='call_id')\n",
        "merged_data = pd.merge(merged_data, sentiment_statistics, on='call_id')\n",
        "merged_data = pd.merge(merged_data, customers, on='customer_id')\n",
        "\n",
        "# Step 4: Further Data Cleaning for merged dataset\n",
        "merged_data.dropna(subset=['primary_call_reason', 'call_transcript'], inplace=True)\n",
        "\n",
        "# Step 5: Feature Engineering\n",
        "merged_data['call_start_datetime'] = pd.to_datetime(merged_data['call_start_datetime'])\n",
        "merged_data['call_end_datetime'] = pd.to_datetime(merged_data['call_end_datetime'])\n",
        "merged_data['call_duration'] = (merged_data['call_end_datetime'] - merged_data['call_start_datetime']).dt.total_seconds()\n",
        "\n",
        "merged_data['call_hour'] = merged_data['call_start_datetime'].dt.hour\n",
        "\n",
        "def get_time_of_day(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'evening'\n",
        "    else:\n",
        "        return 'night'\n",
        "\n",
        "merged_data['time_of_day'] = merged_data['call_hour'].apply(get_time_of_day)\n",
        "\n",
        "# Step 6: Label Encoding for categorical features\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encoding 'customer_tone', 'agent_tone', 'time_of_day'\n",
        "merged_data['customer_tone_encoded'] = label_encoder.fit_transform(merged_data['customer_tone'])\n",
        "merged_data['agent_tone_encoded'] = label_encoder.fit_transform(merged_data['agent_tone'])\n",
        "merged_data['time_of_day_encoded'] = label_encoder.fit_transform(merged_data['time_of_day'])\n",
        "\n",
        "# Step 7: Text Analysis (using TF-IDF for call transcripts)\n",
        "tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "transcript_tfidf = tfidf.fit_transform(merged_data['call_transcript'])\n",
        "\n",
        "transcript_df = pd.DataFrame(transcript_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
        "merged_data = pd.concat([merged_data, transcript_df], axis=1)\n",
        "\n",
        "# Step 8: Model Training and Predictive Analysis\n",
        "features = ['call_duration', 'silence_percent_average', 'customer_tone_encoded', 'agent_tone_encoded','elite_level_code','average_sentiment',] + list(transcript_df.columns)\n",
        "\n",
        "# Fill any missing values in numerical columns with 0 (as a fallback step for model compatibility)\n",
        "merged_data[features] = merged_data[features].fillna(0)\n",
        "\n",
        "X = merged_data[features]\n",
        "y = merged_data['primary_call_reason']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Model Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GW6Lw0FNB0UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load all necessary datasets\n",
        "test_data = pd.read_csv('test.csv')  # test.csv only contains call_id\n",
        "calls_data = pd.read_csv('calls.csv')  # Contains call-related features\n",
        "customers_data = pd.read_csv('customers.csv')  # Contains customer-related features\n",
        "reasons_data = pd.read_csv('reason.csv')  # Contains primary_call_reason\n",
        "sentiment_data = pd.read_csv('sentiment_statistics.csv')  # Sentiment related features\n",
        "\n",
        "# Step 2: Merge test data with other datasets using `call_id` (join on call_id)\n",
        "# This assumes each call_id in the test file exists in calls_data, customers_data, etc.\n",
        "test_data = test_data.merge(calls_data, on='call_id', how='left')\n",
        "test_data = test_data.merge(customers_data, on='customer_id', how='left')\n",
        "test_data = test_data.merge(sentiment_data, on='call_id', how='left')\n",
        "\n",
        "# Step 3: Feature Engineering (same as training data)\n",
        "test_data['call_start_datetime'] = pd.to_datetime(test_data['call_start_datetime'])\n",
        "test_data['call_end_datetime'] = pd.to_datetime(test_data['call_end_datetime'])\n",
        "\n",
        "# Calculate call_duration\n",
        "test_data['call_duration'] = (test_data['call_end_datetime'] - test_data['call_start_datetime']).dt.total_seconds()\n",
        "\n",
        "# Creating call_hour and time_of_day features\n",
        "test_data['call_hour'] = test_data['call_start_datetime'].dt.hour\n",
        "\n",
        "def get_time_of_day(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'evening'\n",
        "    else:\n",
        "        return 'night'\n",
        "\n",
        "test_data['time_of_day'] = test_data['call_hour'].apply(get_time_of_day)\n",
        "\n",
        "# Step 4: Label Encoding for categorical columns ('customer_tone', 'agent_tone', 'time_of_day')\n",
        "label_encoder = LabelEncoder()\n",
        "test_data['customer_tone_encoded'] = label_encoder.fit_transform(test_data['customer_tone'].fillna(''))\n",
        "test_data['agent_tone_encoded'] = label_encoder.fit_transform(test_data['agent_tone'].fillna(''))\n",
        "test_data['time_of_day_encoded'] = label_encoder.fit_transform(test_data['time_of_day'].fillna(''))\n",
        "\n",
        "# Step 5: TF-IDF transformation for the call transcript\n",
        "# Use the same TF-IDF vectorizer that was fitted on training data\n",
        "tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "\n",
        "# For this example, you'd fit the vectorizer using training data (recreate or load the vectorizer)\n",
        "train_transcripts = merged_data['call_transcript'].fillna('')\n",
        "tfidf.fit(train_transcripts)\n",
        "\n",
        "# Transform the test transcripts using the same TF-IDF vectorizer\n",
        "test_transcript_tfidf = tfidf.transform(test_data['call_transcript'].fillna(''))\n",
        "test_transcript_df = pd.DataFrame(test_transcript_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Step 6: Concatenate the test TF-IDF features with the original test DataFrame\n",
        "test_data = pd.concat([test_data, test_transcript_df], axis=1)\n",
        "\n",
        "# Step 7: Ensure that the test dataset contains the same features as the training set\n",
        "features = ['call_duration', 'silence_percent_average', 'customer_tone_encoded', 'agent_tone_encoded','elite_level_code','average_sentiment',] + list(transcript_df.columns)\n",
        "\n",
        "# Since test_data doesn't have some of these features, you can fill them with defaults (e.g., 0)\n",
        "test_data['silence_percent_average'] = test_data['silence_percent_average'].fillna(0)\n",
        "\n",
        "# Ensure no NaNs in the features columns\n",
        "test_data[features] = test_data[features].fillna(0)\n",
        "\n",
        "# Predict the primary call reason for test data\n",
        "test_predictions = model.predict(test_data[features])\n",
        "\n",
        "# Step 9: Save the predictions in the required format\n",
        "output = pd.DataFrame({'call_id': test_data['call_id'], 'primary_call_reason': test_predictions})\n",
        "\n",
        "# Save to CSV\n",
        "output.to_csv('test_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ls3mO2VrFO5I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}